# Team-Data_Science
<h1 align="center">Hi ðŸ‘‹, we are "Team-Data_Science"</h1>
<h3 align="center">A team of 20 members for HackBio'2021 Virtual Bioinformatics Internship</h3>
- ðŸ”­ We are currently working on <a href="https://github.com/Bhushan-Wagh025/Team-Data_Science">**Team-Data_Science**</a>

- ðŸ‘¯ HackBio Channel [https://hackbio-internship.github.io/webpage-test/](https://hackbio-internship.github.io/webpage-test/)

<h3 align="left">Language:</h3>
<a href="https://www.python.org" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> </a>

### Team Work
**To build a model to accurately detect the presence of Parkinsonâ€™s Disease in an individual.**

## Algorithm used initially: 
<h4 align="center">XGBoost</h3>
XGBoost is a new Machine Learning algorithm designed with speed and performance in mind. XGBoost stands for eXtreme Gradient Boosting and is based on decision trees. In this project, we will import the XGBClassifier from the xgboost library; this is an implementation of the scikit-learn API for XGBoost classification.



## Python libraries used:
scikit-learn, numpy, pandas, matplotlib and xgboost

## Python Built in Packages used:
json warnings re

In this Python machine learning project, using the <a href="https://data-flair.training/blogs/python-libraries/">Python libraries</a> scikit-learn, numpy, pandas, and xgboost, we will build a model using an XGBClassifier. Weâ€™ll load the data, get the features and labels, scale the features, then split the dataset, build an XGBClassifier, and then calculate the accuracy of our model.

Dataset used:
Youâ€™ll need the UCI ML Parkinsons dataset for this; you can <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/">download it here.</a> The dataset has 24 columns and 195 records and is only 39.7 KB.


## Other Algorithms which we used to analyse the data, and drew comparisons between the results:

## Beginners:
<h6 align="center"><br>logistic regression<br></h6>

## Intermediate/Advanced:
<h6 align="center">k-nearest neighbors<br>
support vector machines<br>
random forests<br>
bagging algorithms<br>
Naive Bayes</h6>

## Apart from this we generated graphical representation for our data and results:
<h7 align="center">Histograms<br>
Scatter Plot<br>
Heatmaps<br>
Bar Charts<br>
Box<br>
Etc</h7>
